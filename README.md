*Dataset Conversion:*
Converted the Databricks Dolly 15k dataset into a conversational format that works with LLaMA models. You can find the converted dataset here: https://huggingface.co/datasets/Farwah/databricks-dolly-15k_llama_format_dataset

*Fine-tuning llama-2-7b-guanaco:*
Also fully finetuned llama-2-7b-guanaco on instruction data Guanaco-LLaMA2-1k using LoRa by reducing the number of trainable parameters and producing lightweight and efficient models and tested it out through some interactions.
